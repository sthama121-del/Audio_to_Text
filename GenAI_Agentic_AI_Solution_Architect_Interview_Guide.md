# GenAI/Agentic AI Solution Architect - Complete Interview Preparation Guide
## Aerospace Industry Focus

---

## Table of Contents
1. [Job Description Analysis](#job-description-analysis)
2. [Technical Interview Questions & Answers](#technical-interview-questions--answers)
3. [LangSmith Evaluation Deep Dive](#langsmith-evaluation-deep-dive)
4. [AT&T Telecom Policy Example](#att-telecom-policy-example)
5. [Questions to Ask Interviewer](#questions-to-ask-interviewer)
6. [Final Preparation Tips](#final-preparation-tips)

---

# Job Description Analysis

## Must Have Technical/Functional Skills

### Technical Leadership & Solution Delivery (Must-Have Hands-on Skills)
- **GenAI and Agentic AI Expertise**: Design, prototype, and implement cutting-edge solutions leveraging both Generative AI (e.g., LLMs, RAG architectures, prompt engineering) and Agentic AI frameworks (e.g., LangChain, AutoGen) to solve complex business problems.
- **System Architecture**: Demonstrate a deep understanding of how AI models integrate with existing enterprise systems (e.g., ERP, CRM, PLM) to enable seamless, end-to-end business workflows.
- **Proof-of-Concept (POC) Development**: Lead rapid prototyping and minimum viable product (MVP) development to quickly demonstrate technical feasibility and business value to the client.

### Roles & Responsibilities
- **Technical Roadmapping**: Define the technical strategy, architecture, and technology stack for large-scale AI implementations.

### Business Development & Client Engagement (Strong Business Acumen)
- **Opportunity Identification**: Proactively engage with executive-level and technical client stakeholders to understand their core business problems and identify new opportunities where GenAI and Agentic AI can drive significant competitive advantage or cost savings.
- **Requirement Translation**: Masterfully translate ambiguous customer requirements and challenges into clear, actionable technical specifications and quantifiable business outcomes.
- **Business Case Creation**: Develop compelling business cases, ROI analyses, and presentation materials that articulate the value proposition of proposed AI solutions.
- **Aerospace Domain Knowledge (Highly Positive)**: Leverage prior experience in the aerospace industry (e.g., manufacturing, MRO, supply chain, engineering design) to anchor solutions in industry-specific realities and challenges.

### Generic Managerial Skills
- **Stakeholder Management & Soft Skills (Street Smart & Polished)**
- **Executive Communication**: Communicate complex technical concepts to non-technical business leaders and executives with clarity, confidence, and persuasiveness.
- **Relationship Building**: Act as a trusted advisor, building strong, long-term relationships with key client decision-makers.
- **Cross-Functional Collaboration**: Partner effectively with sales, account management, product, and delivery teams to ensure a coordinated and successful go-to-market strategy.
- **Mentorship**: Provide technical guidance and mentorship to internal teams on the application of GenAI and Agentic AI technologies.

---

# Technical Interview Questions & Answers

## SECTION 1: Technical Leadership & GenAI/Agentic AI Expertise

### Question 1: "Walk me through how you would architect a RAG-based solution for our aerospace manufacturing knowledge base that contains technical manuals, maintenance procedures, and engineering specifications."

**Strong Answer:**
"I'd architect this in phases focusing on production readiness:

**Document Processing Layer:**
- Implement multi-modal document loaders supporting PDFs, CAD files, and structured data from PLM systems
- Use RecursiveCharacterTextSplitter with aerospace-specific chunking (1000-1500 tokens with 200 overlap) to preserve technical context
- Apply metadata tagging for document type, aircraft model, revision dates, and compliance requirements

**Embedding & Retrieval:**
- Deploy domain-adapted embeddings, potentially fine-tuning OpenAI embeddings on aerospace terminology
- Implement hybrid search combining FAISS for semantic search with BM25 for exact technical specification matching
- Use metadata filtering to ensure retrieval respects certification levels and document currency

**Agent Architecture:**
- Build a LangChain agent with specialized tools: one for retrieval, one for calculation validation, one for compliance checking
- Implement guardrails to prevent hallucination on safety-critical information
- Add citation mechanisms to trace answers back to specific document sections and revisions

**Integration:**
- REST APIs to connect with existing ERP/PLM systems like SAP or Siemens Teamcenter
- Real-time document sync pipelines to keep knowledge base current
- Audit logging for regulatory compliance (AS9100, FAA requirements)"

---

### Question 2: "Explain the difference between GenAI and Agentic AI, and give me a real-world aerospace use case for each."

**Strong Answer:**
"**GenAI** focuses on content generation using LLMs‚Äîcreating, summarizing, or transforming information. **Agentic AI** goes further by making autonomous decisions, using tools, and executing multi-step workflows.

**GenAI Use Case - Aerospace:**
Generating customized maintenance reports. An LLM takes structured inspection data, historical maintenance records, and generates human-readable reports with recommendations‚Äîbut a human validates and approves actions.

**Agentic AI Use Case - Aerospace:**
Autonomous supply chain optimization agent. The agent:
1. Monitors inventory levels across multiple warehouses
2. Predicts part failures using historical maintenance data
3. Automatically generates purchase orders when thresholds are hit
4. Negotiates with suppliers via API integration
5. Updates ERP systems and notifies procurement teams

The agent uses LangChain with tools for inventory queries, predictive models, supplier APIs, and ERP integration‚Äîmaking decisions and taking actions within defined parameters."

---

### Question 3: "You mentioned building a knowledge assistant pipeline. What were the biggest technical challenges, and how did you solve them?"

**Strong Answer:**
"Three major challenges emerged:

**1. Dependency Management in Colab:**
- Challenge: LangChain's rapid evolution caused import structure changes
- Solution: Pinned specific package versions, migrated from deprecated `langchain.document_loaders` to `langchain_community.document_loaders`, maintained compatibility matrix

**2. Chunking Strategy for Research Papers:**
- Challenge: Standard chunking broke mathematical equations and figure references
- Solution: Implemented custom splitters that respect section boundaries, preserve equations as atomic units, and maintain cross-references through metadata linking

**3. Retrieval Quality:**
- Challenge: FAISS alone missed contextually relevant but semantically different passages
- Solution: Implemented ensemble retrieval combining dense (FAISS) and sparse (BM25) methods, then re-ranked results using cross-encoder models for final precision"

---

## SECTION 2: Solution Architecture & System Integration

### Question 4: "How would you integrate a GenAI solution with an existing aerospace ERP system like SAP to automate parts procurement recommendations?"

**Strong Answer:**
"I'd design a five-layer architecture:

**1. Data Integration Layer:**
- Connect to SAP via RFC/BAPI or OData services to pull real-time inventory, BOM data, and historical consumption
- ETL pipeline to sync SAP master data into vector database with proper normalization

**2. Intelligence Layer:**
- RAG system ingesting supplier catalogs, historical purchase orders, quality reports
- Predictive models for demand forecasting and lead time estimation
- LLM agent with tools to query SAP, analyze trends, and generate recommendations

**3. Decision Engine:**
- Business rules engine validating recommendations against procurement policies, budget constraints, and approved vendor lists
- Confidence scoring to flag recommendations needing human review

**4. Action Layer:**
- API endpoints to SAP MM module to create purchase requisitions
- Approval workflow integration with SAP's existing authorization hierarchy

**5. Observability:**
- Logging all AI decisions with explanations for audit trails
- Performance dashboards showing cost savings, lead time improvements, and forecast accuracy

**Key Consideration:** In aerospace, I'd implement a human-in-the-loop for the first 6 months, with the AI suggesting and humans approving, gradually increasing automation as trust builds."

---

### Question 5: "Describe your approach to building a POC for an Agentic AI solution. What's your timeline and success metrics?"

**Strong Answer:**
"**Week 1-2: Discovery & Scoping**
- Stakeholder workshops to identify high-impact use case
- Data availability assessment
- Define 2-3 measurable success criteria (e.g., 'reduce manual research time by 60%')

**Week 3-4: Rapid Prototyping**
- Build MVP with LangChain/AutoGen on sample data
- Core agent with 2-3 essential tools
- Basic UI (Streamlit) for stakeholder demos

**Week 5-6: Refinement & Validation**
- User testing with actual end-users
- Measure against success metrics
- Document technical architecture and cost projections

**Success Metrics I Typically Track:**
1. **Accuracy:** % of correct responses (target: >85% for POC)
2. **Efficiency:** Time saved vs manual process
3. **User Adoption:** Willingness to use in daily work (survey feedback)
4. **Technical Feasibility:** Token costs, latency, scalability assessment

**Deliverables:**
- Working prototype
- ROI analysis with production cost estimates
- Technical architecture document
- Pilot-to-production roadmap"

---

## SECTION 3: Business Acumen & Opportunity Identification

### Question 6: "An aerospace MRO client says their maintenance technicians spend 40% of their time searching for technical documentation. How would you approach this opportunity?"

**Strong Answer:**
"**Step 1: Quantify the Business Impact**
- Calculate: If 100 technicians at $75/hour spend 40% searching = 16,000 hours/year = $1.2M annual cost
- Add downstream impacts: aircraft downtime, delayed flights, customer satisfaction

**Step 2: Root Cause Analysis**
- Conduct user interviews: What makes search hard? (outdated indexes, fragmented systems, complex terminology)
- Data audit: How many documents? What formats? How frequently updated?

**Step 3: Solution Design**
- Propose conversational AI assistant with:
  - Natural language query: 'How do I replace the hydraulic pump on a 737-800?'
  - Multi-source retrieval: OEM manuals, internal SOPs, historical work orders
  - Visual aids: Return diagrams, part photos, video links
  - Mobile-first design for hangar floor access

**Step 4: Business Case**
- **Investment:** $150K for POC, $500K for production deployment
- **ROI:** Save 60% of search time = $720K annual savings
- **Payback:** 8 months
- **Strategic value:** Reduce aircraft ground time, improve technician training, capture tribal knowledge

**Step 5: De-risk with POC**
- 4-week pilot with 20 technicians on one aircraft type
- Measure search time reduction and user satisfaction
- Build confidence before full rollout

I'd present this with a live demo, customer testimonials from similar deployments, and a phased implementation roadmap."

---

### Question 7: "How do you translate a vague customer requirement like 'We want to use AI to improve our supply chain' into a concrete technical solution?"

**Strong Answer:**
"I use a structured discovery framework:

**1. Socratic Questioning:**
- 'What specific supply chain challenges cost you the most?' (lead time variability, stock-outs, excess inventory)
- 'Where do decisions get bottlenecked?' (demand planning, supplier selection, expediting)
- 'What data do you have?' (ERP records, supplier performance, market signals)

**2. Pain Point Prioritization:**
- Map pain points to business impact (revenue loss, cost, compliance risk)
- Assess data readiness and technical feasibility
- Select 1-2 high-impact, achievable targets

**3. Solution Translation:**
Example: 'Excess inventory' becomes:
- **Technical Spec:** Demand forecasting agent using historical sales, market trends, and promotional calendars to optimize reorder points
- **Data Requirements:** 3 years of transactional data, supplier lead times, seasonality patterns
- **Success Metric:** Reduce inventory carrying costs by 15% while maintaining 98% fill rate

**4. Proof of Concept Proposal:**
- 6-week pilot on one product category
- Mock-up of agent workflow and decision dashboard
- Risk mitigation: Human oversight on all automated decisions initially

**5. Stakeholder Alignment:**
- Present to CFO (cost savings), COO (efficiency), CIO (integration requirements)
- Build consensus on success criteria before starting

This approach converts 'We want AI' into 'We will deploy a demand forecasting agent that reduces inventory costs by $X while maintaining service levels.'"

---

## SECTION 4: Aerospace Domain Knowledge

### Question 8: "What aerospace-specific challenges would you consider when designing AI solutions for this industry?"

**Strong Answer:**
"Aerospace has unique constraints that shape AI solution design:

**1. Regulatory Compliance:**
- FAA, EASA regulations require explainability and audit trails
- AI recommendations on safety-critical tasks need human validation
- Design: Implement citation mechanisms, decision logging, and override capabilities

**2. Data Sensitivity & IP Protection:**
- Proprietary designs, ITAR-controlled technical data
- Design: On-premise or private cloud deployment, data encryption, access controls aligned with security clearances

**3. Long Product Lifecycles:**
- Aircraft operate 20-30 years with evolving maintenance requirements
- Design: Version control for knowledge bases, backward compatibility, easy updates as new service bulletins released

**4. Precision & Zero-Tolerance for Error:**
- Wrong part specification could cause safety incidents
- Design: Confidence thresholds, multi-model consensus, human-in-loop for critical decisions

**5. Interoperability with Legacy Systems:**
- Many aerospace companies run SAP, Siemens Teamcenter, older CMMS platforms
- Design: API-first architecture, standard integration protocols, graceful degradation

**Example Application:**
For an AI-powered maintenance advisor, I'd ensure every recommendation includes:
- Source documentation with revision numbers
- Compliance verification against current airworthiness directives
- Confidence score with human review flags for <90% confidence
- Full audit log for post-incident investigation"

---

## SECTION 5: Stakeholder Management & Communication

### Question 9: "How would you explain the value of RAG architecture to a non-technical aerospace executive who's skeptical about AI?"

**Strong Answer:**
"I'd use an analogy rooted in their world:

'Think of your most experienced engineer who's been here 30 years. When a complex problem arises, they don't rely on memory alone‚Äîthey go to their file cabinet, pull out relevant manuals, reference past projects, and then synthesize an answer based on proven documentation.

RAG works the same way, but at scale. It:
1. **Searches** your entire knowledge base instantly (like that engineer's file cabinet, but 10,000x faster)
2. **Retrieves** only the most relevant documents (quality, not quantity)
3. **Generates** an answer using that verified information (not making things up)

**Why this matters for you:**
- Your top 10% of engineers hold critical knowledge. RAG makes that expertise available to everyone, 24/7.
- When those experts retire, their knowledge doesn't walk out the door.
- New engineers get accurate answers in minutes, not days of searching or waiting for expert availability.

**Risk mitigation:**
Unlike pure AI that can hallucinate, RAG always shows its sources. If it says "replace part X every 500 hours," it shows you the exact maintenance manual and page number.

**Pilot proposal:**
Let's test it on your most time-consuming technical query type‚Äîmaintenance troubleshooting. Measure time-to-resolution before and after. If it doesn't save 50% of search time in 30 days, we pause and reassess.'

This approach demystifies the technology, connects to tangible business value, and reduces perceived risk with a controlled pilot."

---

### Question 10: "Tell me about a time you had to navigate conflicting stakeholder priorities in a technical project."

**Strong Answer (Use STAR Format):**
"**Situation:**
In my knowledge assistant project, I had three stakeholders: research team wanted deep technical accuracy, management wanted fast deployment for a demo, and IT wanted strict security controls.

**Task:**
Deliver a working prototype in 4 weeks that satisfied all three priorities.

**Action:**
1. **Alignment Workshop:** I facilitated a 2-hour session where each stakeholder ranked their top 3 priorities. Found common ground: everyone agreed on 'accurate answers' being non-negotiable.

2. **Phased Approach:**
   - Week 1-2: Build core RAG pipeline with robust chunking and retrieval (research team's priority)
   - Week 3: Fast UI development with Streamlit for demo (management's priority)
   - Week 4: Implement API authentication and data access controls (IT's priority)

3. **Transparent Communication:**
   - Daily Slack updates on progress
   - Friday demos showing incremental value
   - Explicitly documented trade-offs (e.g., 'deprioritized advanced re-ranking for Week 5 to meet security requirements')

4. **Compromise:**
   - Research team accepted using OpenAI API (cloud) for POC with commitment to migrate to on-premise models for production
   - Management accepted later demo date (pushed 1 week) to ensure security wasn't rushed

**Result:**
Delivered on Week 5 with 95% stakeholder satisfaction. Research team validated 87% answer accuracy, management successfully demoed to executives, IT approved for pilot deployment. Built trust that led to production budget approval."

---

## SECTION 6: Mentorship & Team Leadership

### Question 11: "How would you upskill a traditional software engineering team to work effectively with GenAI and Agentic AI technologies?"

**Strong Answer:**
"**Phase 1: Foundation Building (Weeks 1-2)**
- **Hands-on Workshop:** Build a simple RAG app together using LangChain in 4 hours
- **Conceptual Training:** LLMs vs traditional ML, prompt engineering basics, when to use GenAI vs classical approaches
- **Resources:** Curate learning paths (DeepLearning.AI courses, LangChain docs, Anthropic's prompt engineering guide)

**Phase 2: Learning by Doing (Weeks 3-8)**
- **Pair Programming:** Assign each engineer a buddy for their first GenAI feature
- **Real Project Assignments:** Small, scoped tasks like 'implement document summarization for customer support tickets'
- **Code Reviews:** Review prompts as rigorously as code, discuss retrieval strategies, evaluate output quality

**Phase 3: Best Practices & Patterns (Ongoing)**
- **Internal Knowledge Sharing:** Bi-weekly 'GenAI office hours' where team discusses challenges
- **Build Reusable Components:** Create shared libraries for common patterns (RAG pipelines, agent templates, evaluation frameworks)
- **Experiment Culture:** Dedicate 10% time for engineers to prototype ideas, share learnings

**Phase 4: Production Readiness**
- **Training on:** Prompt injection prevention, PII handling, cost optimization, latency management
- **Establish Standards:** Evaluation metrics (accuracy, latency, cost), monitoring dashboards, incident response

**Mentorship Style:**
I believe in 'teach a person to fish'‚Äîgive engineers autonomy to explore, create psychological safety to fail, and celebrate learning. I'd run monthly 'demo days' where anyone can show experiments, even failed ones, to normalize iteration."

---

# LangSmith Evaluation Deep Dive

## How LangSmith Determines "Right" vs "Wrong" Answers

LangSmith doesn't automatically "know" what's right‚Äî**you have to teach it**. Here's how:

---

## 1. Ground Truth Approaches

### A. Human-Labeled Datasets (Most Common)

```python
from langsmith import Client

client = Client()

# You create a dataset with expected answers
examples = [
    {
        "inputs": {"question": "What is the capital of France?"},
        "outputs": {"answer": "Paris"}  # Ground truth
    },
    {
        "inputs": {"question": "When was the Eiffel Tower built?"},
        "outputs": {"answer": "1889"}  # Ground truth
    }
]

# Upload to LangSmith
dataset = client.create_dataset("geography_qa")
for example in examples:
    client.create_example(
        inputs=example["inputs"],
        outputs=example["outputs"],
        dataset_id=dataset.id
    )
```

**How it works:**
- You manually curate expected answers
- LangSmith compares your RAG system's output against these ground truths
- **Limitation:** Labor-intensive; doesn't scale to thousands of questions

---

### B. LLM-as-Judge (Automated Evaluation)

Since you can't manually label thousands of examples, you use **another LLM to evaluate** the first LLM's answer:

```python
from langsmith.evaluation import evaluate, LangChainStringEvaluator

# Define evaluator using GPT-4 as judge
evaluator = LangChainStringEvaluator(
    "qa",  # Question-answering task
    config={
        "criteria": {
            "accuracy": "Is the answer factually correct based on the context?",
            "completeness": "Does it fully answer the question?",
            "conciseness": "Is it free of unnecessary information?"
        }
    }
)

# Run evaluation
results = evaluate(
    lambda inputs: my_rag_chain.invoke(inputs),  # Your RAG system
    data="geography_qa",  # Dataset name
    evaluators=[evaluator]
)
```

**How GPT-4 judges:**
```
System: You are evaluating a RAG system's answer.

Context (from retrieved docs): "Paris is the capital of France, 
located in the north-central part of the country."

Question: "What is the capital of France?"

Generated Answer: "The capital of France is Paris."

Rate 0-1 on:
- Accuracy: Does the answer match the retrieved context?
- Hallucination: Any fabricated information?
```

**Pros:** Scales to thousands of examples  
**Cons:** Judge LLM can also be wrong; costs money per evaluation

---

### C. Reference-Free Metrics (No Ground Truth Needed)

For some checks, you don't need "right answers":

```python
from langsmith.evaluation import evaluate

# Check if context was actually used
def context_relevance_check(run, example):
    """Check if retrieved chunks are relevant to the question"""
    retrieved_docs = run.outputs.get("source_documents", [])
    question = run.inputs["question"]
    
    # Simple heuristic: do retrieved docs contain question keywords?
    question_terms = set(question.lower().split())
    doc_terms = set(" ".join([doc.page_content for doc in retrieved_docs]).lower().split())
    
    overlap = len(question_terms & doc_terms) / len(question_terms)
    return {"key": "context_relevance", "score": overlap}

evaluate(
    my_rag_chain,
    data="test_set",
    evaluators=[context_relevance_check]
)
```

---

## 2. Traceability in LangSmith

LangSmith's **tracing** shows you the full execution path:

### Example Trace Visualization:

```
Run ID: abc123
‚îú‚îÄ Input: "What maintenance interval for Boeing 737 hydraulic pump?"
‚îú‚îÄ Step 1: Document Retrieval [120ms]
‚îÇ   ‚îú‚îÄ Query Embedding: [0.23, -0.45, ...] 
‚îÇ   ‚îú‚îÄ FAISS Search Results:
‚îÇ   ‚îÇ   ‚îú‚îÄ Doc 1: "737-800 Hydraulic System Maintenance" (score: 0.89)
‚îÇ   ‚îÇ   ‚îú‚îÄ Doc 2: "Pump Replacement Procedures" (score: 0.76)
‚îÇ   ‚îÇ   ‚îî‚îÄ Doc 3: "General Maintenance Schedule" (score: 0.68)
‚îÇ   ‚îî‚îÄ Metadata: {aircraft_model: "737-800", doc_type: "maintenance_manual"}
‚îú‚îÄ Step 2: LLM Generation [2.3s]
‚îÇ   ‚îú‚îÄ Prompt: 
‚îÇ   ‚îÇ   "Based on these documents, answer:
‚îÇ   ‚îÇ   [Doc 1 content...]
‚îÇ   ‚îÇ   Question: What maintenance interval..."
‚îÇ   ‚îú‚îÄ Model: gpt-4-turbo
‚îÇ   ‚îú‚îÄ Tokens: 1,250 prompt / 85 completion
‚îÇ   ‚îî‚îÄ Raw Output: "The hydraulic pump should be inspected every 500 flight hours..."
‚îî‚îÄ Final Output: "Every 500 flight hours"

Evaluation Results:
‚úì Context Relevance: 0.92
‚úó Factual Accuracy: 0.40 (Expected: 600 hours, Got: 500 hours)
‚úì Citation Present: Yes
```

---

## 3. Debugging Wrong Answers (Traceability Investigation)

### Scenario: Wrong Answer Detected

**Question:** "What is the inspection interval for landing gear?"  
**Expected:** "Every 1000 flight cycles"  
**Your RAG Said:** "Every 500 flight hours"

### Step 1: Check Retrieval Quality
```python
# In LangSmith UI or API
trace = client.read_run(run_id="abc123")

# Inspect retrieved documents
retrieved_docs = trace.outputs["source_documents"]
for i, doc in enumerate(retrieved_docs):
    print(f"Doc {i}: Score={doc.metadata['score']}")
    print(f"Content: {doc.page_content[:200]}...")
```

**‚ùå Problem Found:** Retrieved documents are about "hydraulic systems" not "landing gear"  
**Root Cause:** Query embedding didn't capture "landing gear" semantically

**Fix:** Improve chunking to keep "landing gear" and "inspection interval" together, or use hybrid search

---

### Step 2: Check Prompt Construction
```python
# View the exact prompt sent to LLM
print(trace.inputs["prompt"])
```

**Output:**
```
Based on the following documents:
Document 1: [Hydraulic pump maintenance every 500 hours...]
Document 2: [General inspection guidelines...]

Question: What is the inspection interval for landing gear?
```

**‚ùå Problem Found:** Prompt doesn't explicitly say "only answer from provided documents"  
**Root Cause:** LLM used its training data instead of retrieved context

**Fix:** Add to prompt: "If the answer is not in the documents, say 'Information not found in provided documentation.'"

---

### Step 3: Check LLM Output Reasoning
```python
# If using chain-of-thought
print(trace.outputs["reasoning"])
```

**Output:**
```
The documents mention hydraulic systems with 500-hour intervals. 
While the question asks about landing gear, I'll apply similar 
maintenance principles...
```

**‚ùå Problem Found:** LLM is hallucinating/extrapolating  
**Fix:** Use stronger prompt constraints or post-processing to validate answers cite retrieved docs

---

## 4. Practical Evaluation Pipeline

```python
from langsmith import Client, evaluate
from langsmith.evaluation import LangChainStringEvaluator

client = Client()

# 1. Create test dataset (ground truth)
test_cases = [
    {
        "question": "What is the landing gear inspection interval?",
        "expected_answer": "Every 1000 flight cycles",
        "source_doc_id": "AMM-32-10-05"  # Exact document that should be retrieved
    },
    # ... 100+ more test cases
]

# 2. Define multi-level evaluators
evaluators = [
    # A. Retrieval Quality
    lambda run: {
        "key": "correct_doc_retrieved",
        "score": 1 if run.outputs["source_doc_ids"][0] == run.reference["source_doc_id"] else 0
    },
    
    # B. Answer Accuracy (LLM-as-judge)
    LangChainStringEvaluator("qa", config={
        "criteria": "Does the answer match the expected answer?"
    }),
    
    # C. Citation Check
    lambda run: {
        "key": "has_citation",
        "score": 1 if "AMM-" in run.outputs["answer"] else 0
    },
    
    # D. Hallucination Detection
    lambda run: {
        "key": "no_hallucination",
        "score": check_answer_uses_only_retrieved_context(
            run.outputs["answer"], 
            run.outputs["source_documents"]
        )
    }
]

# 3. Run evaluation
results = evaluate(
    my_rag_pipeline,
    data=test_cases,
    evaluators=evaluators
)

# 4. Analyze failures
print(f"Retrieval Accuracy: {results['correct_doc_retrieved'].mean()}")
print(f"Answer Accuracy: {results['qa'].mean()}")
print(f"Citation Rate: {results['has_citation'].mean()}")
```

---

## 5. Advanced: Custom Aerospace Evaluator

```python
def aerospace_compliance_evaluator(run, example):
    """Check if answer meets aerospace documentation standards"""
    answer = run.outputs["answer"]
    sources = run.outputs.get("source_documents", [])
    
    checks = {
        "has_document_reference": bool(re.search(r'AMM-\d+-\d+-\d+', answer)),
        "has_revision_date": bool(re.search(r'\d{4}-\d{2}-\d{2}', answer)),
        "uses_approved_terminology": check_terminology_database(answer),
        "cites_current_revision": all(
            doc.metadata["revision"] == get_latest_revision(doc.metadata["doc_id"]) 
            for doc in sources
        ),
        "safety_critical_flag": "WARNING" in answer.upper() if example["safety_critical"] else True
    }
    
    score = sum(checks.values()) / len(checks)
    return {
        "key": "aerospace_compliance",
        "score": score,
        "details": checks
    }
```

---

## 6. Limitations You Should Know

### 1. Ground Truth is Expensive
- Manual labeling: ~5 min per example
- 1000 test cases = 83 hours of expert time
- **Solution:** Start with 50-100 high-priority questions, expand over time

### 2. LLM-as-Judge Can Be Wrong
- Judge might agree with wrong answer if it "sounds good"
- **Solution:** Use multiple judges, human spot-checks on 10% of evaluations

### 3. Context Matters
- "Every 500 hours" might be right for one aircraft model, wrong for another
- **Solution:** Include metadata in evaluation (aircraft type, regulation version)

### 4. Ambiguous Questions
- "When should I replace the pump?" could mean preventive maintenance OR failure response
- **Solution:** Rewrite test questions to be unambiguous

---

# AT&T Telecom Policy Example

## Perfect Interview Answer: LangSmith Evaluation for AT&T Telecom Policy RAG System

**Interviewer:** "How does LangSmith evaluate if your RAG system is giving correct answers?"

---

### Your Response:

"Great question. Let me explain with a concrete example from telecom policy domain.

---

## The Scenario

Imagine we're building a RAG system for AT&T customer service agents to query internal policy documents.

**User Question:** *'What is the refund policy for early contract termination on a business unlimited plan?'*

**Expected Answer (from policy doc):** *'Business customers terminating before 24 months pay a prorated ETF of $15 per remaining month, minus any device subsidies already recovered. Refunds are processed within 2-3 billing cycles.'*

Now, LangSmith doesn't magically know this is the 'right' answer. **We have to teach it** through a four-level evaluation framework:

---

## Level 1: Ground Truth Dataset (The Foundation)

First, we create a **test dataset** with questions and verified correct answers:

```
Test Case #47:
‚îú‚îÄ Input Question: "What is the refund policy for early contract 
‚îÇ  termination on a business unlimited plan?"
‚îÇ
‚îú‚îÄ Expected Answer: "Business customers pay $15/month prorated 
‚îÇ  ETF for remaining contract period. Refunds processed in 2-3 
‚îÇ  billing cycles."
‚îÇ
‚îú‚îÄ Source Document: "AT&T Business Policy Manual v8.2, 
‚îÇ  Section 4.3.2 - Early Termination Fees"
‚îÇ
‚îî‚îÄ Metadata: 
    - Policy effective date: Jan 2024
    - Customer segment: Business
    - Plan type: Unlimited
```

**How we built this:**
- Subject matter experts (policy team) manually curated 200 critical policy questions
- Documented the exact expected answers and source sections
- This becomes our 'ground truth' baseline

---

## Level 2: Automated Evaluation Metrics

LangSmith runs your RAG system against this test dataset and measures:

### A. Retrieval Accuracy (Did we fetch the right policy document?)

```python
Trace in LangSmith:
‚îú‚îÄ Query: "refund policy early termination business unlimited"
‚îú‚îÄ Retrieved Documents:
‚îÇ   ‚îú‚îÄ Doc 1: "Business ETF Policy" (similarity: 0.91) ‚úì CORRECT
‚îÇ   ‚îú‚îÄ Doc 2: "Consumer ETF Policy" (similarity: 0.78) ‚úó WRONG SEGMENT
‚îÇ   ‚îî‚îÄ Doc 3: "Device Return Policy" (similarity: 0.65) ‚úó IRRELEVANT
‚îÇ
‚îî‚îÄ Evaluation: Correct doc retrieved at rank 1 ‚úì
```

**Metric:** Retrieval Precision@1 = 100% (correct doc in top position)

---

### B. Answer Correctness (LLM-as-Judge)

Since we can't manually check thousands of answers, we use **GPT-4 as an automated judge**:

```python
Judge Prompt to GPT-4:
"""
You are evaluating a customer service AI assistant.

REFERENCE ANSWER (Ground Truth):
"Business customers pay $15/month prorated ETF for remaining 
contract period. Refunds processed in 2-3 billing cycles."

RETRIEVED CONTEXT (What the RAG system found):
[Full text of Business ETF Policy document...]

GENERATED ANSWER (What the RAG system said):
"Business unlimited plan customers who terminate early will be 
charged $15 for each remaining month. Refund processing takes 
2-3 billing cycles."

Rate 0-1 on these criteria:
1. Factual Accuracy: Are all facts correct per the policy?
2. Completeness: Are key details present (ETF amount, timeframe)?
3. No Hallucination: Is everything stated found in the retrieved context?
4. No Conflation: Does it avoid mixing business and consumer policies?

Scoring:
- Factual Accuracy: 1.0 ‚úì (ETF amount, timeframe correct)
- Completeness: 0.9 ‚úì (minor: didn't mention device subsidy caveat)
- No Hallucination: 1.0 ‚úì (all claims verified in context)
- No Conflation: 1.0 ‚úì (correctly scoped to business plans)

Overall Score: 0.975 (97.5% accurate)
"""
```

**Why LLM-as-Judge?**
- Scales to evaluate 1000+ test cases automatically
- Catches semantic correctness (not just exact word matching)
- Can understand domain nuances (business vs consumer policies)

---

### C. Domain-Specific Validators (Telecom Policy Rules)

For telecom, we add **custom business logic checks**:

```python
Telecom Policy Validator:
‚îú‚îÄ ‚úì Contains monetary amount ($15)
‚îú‚îÄ ‚úì Specifies customer segment (business)
‚îú‚îÄ ‚úì Includes timeframe (2-3 billing cycles)
‚îú‚îÄ ‚úì References current policy version (v8.2, Jan 2024)
‚îú‚îÄ ‚úó Missing: Device subsidy caveat (partial completeness)
‚îî‚îÄ ‚úì No conflation with consumer policies

Compliance Score: 5/6 = 83%
```

---

### D. Hallucination Detection (Critical for Policy)

This checks: **"Did the answer invent information not in the retrieved documents?"**

```python
Hallucination Check:
‚îú‚îÄ Claim 1: "$15 per month" 
‚îÇ   ‚îî‚îÄ Found in Doc 1, Section 4.3.2 ‚úì
‚îú‚îÄ Claim 2: "2-3 billing cycles"
‚îÇ   ‚îî‚îÄ Found in Doc 1, Section 4.3.5 ‚úì
‚îú‚îÄ Claim 3: "Business customers"
‚îÇ   ‚îî‚îÄ Found in Doc 1, Title ‚úì
‚îî‚îÄ No unsupported claims detected ‚úì

Hallucination Score: 0% (lower is better)
```

**Why This Matters in Telecom:**
If the system says *"refunds within 24 hours"* but the policy says *"2-3 billing cycles,"* that's a hallucination that could create customer disputes.

---

## Level 3: Accuracy Threshold & Pass/Fail

We set **minimum acceptance criteria**:

```
Test Case #47 Results:
‚îú‚îÄ Retrieval Accuracy: 100% ‚úì (Pass: >90%)
‚îú‚îÄ Answer Correctness: 97.5% ‚úì (Pass: >90%)
‚îú‚îÄ Policy Compliance: 83% ‚ö†Ô∏è (Warning: <85%, Missing details)
‚îú‚îÄ Hallucination: 0% ‚úì (Pass: <5%)
‚îî‚îÄ Overall: PASS with Warning

Action: Flag for manual review of completeness
```

**If Overall Score < 85%:** System fails evaluation; requires debugging.

---

## Level 4: Traceability (Debugging Wrong Answers)

When LangSmith detects a wrong answer, it shows you **exactly where it failed**:

### Example: Failed Test Case

**Question:** *"What's the data overage charge for business plans?"*  
**Expected:** *"$10 per GB over plan limit"*  
**System Said:** *"$15 per GB"* ‚ùå

**LangSmith Trace Investigation:**

```
Run ID: xyz789 - FAILED (Accuracy: 45%)

‚îú‚îÄ Step 1: Document Retrieval
‚îÇ   ‚îú‚îÄ Query Embedding: [0.12, -0.34, ...]
‚îÇ   ‚îú‚îÄ Retrieved Docs:
‚îÇ   ‚îÇ   ‚îú‚îÄ Doc 1: "Consumer Overage Charges" (score: 0.88) ‚ö†Ô∏è WRONG
‚îÇ   ‚îÇ   ‚îî‚îÄ Doc 2: "Business Data Plans" (score: 0.74)
‚îÇ   ‚îî‚îÄ ROOT CAUSE FOUND: Retrieved consumer policy instead of business
‚îÇ
‚îú‚îÄ Step 2: LLM Generation
‚îÇ   ‚îú‚îÄ Prompt: "Based on: [Consumer policy text with $15/GB]..."
‚îÇ   ‚îî‚îÄ Generated: "$15 per GB" (correctly used retrieved doc, but doc was wrong)
‚îÇ
‚îî‚îÄ Diagnosis: Retrieval failure due to insufficient metadata filtering
   Fix: Add customer_segment='business' filter to vector search
```

**This is the power of traceability:** You know it's not the LLM's fault‚Äîthe retrieval layer fetched the wrong policy document.

---

## Complete Evaluation Pipeline (What I'd Build)

```python
# Pseudocode for AT&T Policy RAG Evaluation

# 1. Create test dataset
test_cases = load_policy_questions_with_ground_truth()  # 200 questions

# 2. Run RAG system on all test cases
results = []
for test_case in test_cases:
    # System generates answer
    trace = rag_system.invoke(test_case.question)
    
    # Run evaluations
    retrieval_score = evaluate_retrieval(trace, test_case.source_doc)
    accuracy_score = llm_judge_accuracy(trace.answer, test_case.expected)
    compliance_score = check_policy_compliance(trace)
    hallucination_score = detect_hallucination(trace)
    
    results.append({
        "question": test_case.question,
        "retrieval": retrieval_score,
        "accuracy": accuracy_score,
        "compliance": compliance_score,
        "hallucination": hallucination_score,
        "pass": all_scores_above_threshold()
    })

# 3. Generate report
print(f"Overall Accuracy: {mean(results.accuracy)}")
print(f"Pass Rate: {sum(r.pass for r in results) / len(results)}")
print(f"Failed Cases: {[r for r in results if not r.pass]}")
```

---

## Real-World Numbers (What You'd See)

After running 200 test cases:

```
LangSmith Evaluation Summary:
‚îú‚îÄ Overall Accuracy: 92.3%
‚îú‚îÄ Pass Rate: 184/200 (92%)
‚îú‚îÄ Retrieval Accuracy: 96.5%
‚îú‚îÄ Hallucination Rate: 2.1%
‚îî‚îÄ Policy Compliance: 89.7%

Failed Cases (16):
‚îú‚îÄ 8 cases: Wrong document retrieved (metadata filtering issue)
‚îú‚îÄ 5 cases: Incomplete answers (missing critical caveats)
‚îî‚îÄ 3 cases: Hallucinated timeframes not in policy

Action Items:
1. Improve metadata filtering for customer segment
2. Adjust prompt to include all policy caveats
3. Add stricter hallucination guards
```

---

## Why This Approach Works for Enterprise Telecom

**1. Regulatory Compliance:** Every answer is traceable to source policy document with version

**2. Scalability:** Automated eval runs nightly; catches regressions when policies update

**3. Trust:** Customer service managers can audit failed cases before deployment

**4. Continuous Improvement:** Each failed case teaches us how to improve retrieval or prompts

---

## The Punch Line (End Your Answer With This)

'So to directly answer your question: **LangSmith doesn't know what's right by itself.** We teach it by:

1. **Creating ground truth datasets** curated by policy experts
2. **Using GPT-4 as an automated judge** to scale evaluation to thousands of questions
3. **Adding domain-specific validators** for telecom policy compliance
4. **Detecting hallucinations** by verifying every claim exists in retrieved documents
5. **Setting accuracy thresholds** (e.g., must achieve >90% to pass)

And when something goes wrong, **LangSmith's traceability** shows us exactly which component failed‚Äîretrieval, prompt engineering, or generation‚Äîso we can fix it systematically.

For AT&T's policy system, this meant we could confidently deploy knowing we had 92%+ accuracy on business-critical policy questions, with full audit trails for compliance.'"

---

## Bonus: If They Ask a Follow-Up

**Q:** "What if the policy document changes? How do you re-evaluate?"

**A:** "Excellent question. We have a **continuous evaluation pipeline**:

1. When policy team updates a document (e.g., ETF changes from $15 to $12), they flag it in our system
2. Automated pipeline re-runs all test cases referencing that policy section
3. We update ground truth answers in the test dataset
4. LangSmith re-evaluates‚Äîif accuracy drops below 90%, deployment is blocked until fixed
5. This ensures we catch regressions immediately when source documents change

We also version our test datasets alongside policy versions, so we can prove our system was accurate for the policy version effective at any point in time‚Äîcritical for regulatory audits."

---

# Questions to Ask Interviewer

1. "What's the most pressing GenAI opportunity you're currently exploring with your aerospace clients?"

2. "How does your organization balance innovation speed with the regulatory compliance requirements inherent in aerospace?"

3. "What does success look like for this role in the first 6 months?"

4. "Can you describe a recent Agentic AI project your team delivered? What worked well and what would you do differently?"

5. "How do you see the GenAI/Agentic AI practice evolving over the next 2-3 years within your organization?"

6. "What's the biggest technical challenge your team is facing in deploying GenAI solutions at scale?"

7. "How does your organization approach the build vs. buy decision for GenAI capabilities?"

---

# Final Preparation Tips

## Given Your Hands-on LangChain Experience:

‚úÖ **Emphasize:** Your practical implementation skills (chunking strategies, vector databases, embedding choices)

‚úÖ **Prepare:** A portfolio piece‚Äîhave your research assistant pipeline ready to walk through architecture on a whiteboard or in a screen share

‚úÖ **Connect to Aerospace:** Research 2-3 current aerospace AI initiatives (Boeing's digital twin work, Airbus's AI-powered design optimization) to show domain interest

‚úÖ **Business Mindset:** Practice translating every technical capability into business value (time saved, cost reduced, risk mitigated)

---

## Key Talking Points Summary:

### Technical Strengths to Highlight:
- Hands-on experience with LangChain, RAG architectures, FAISS, OpenAI embeddings
- Understanding of chunking strategies and retrieval optimization
- Knowledge of evaluation frameworks (LangSmith)
- Experience with Google Colab, dependency management, debugging

### Business Acumen to Demonstrate:
- Ability to quantify ROI and business impact
- Understanding of stakeholder management
- Experience translating technical solutions to business value
- Knowledge of compliance and regulatory requirements

### Communication Skills:
- Use analogies appropriate for aerospace (engineer's file cabinet for RAG)
- STAR format for behavioral questions
- Confidence in explaining complex concepts simply
- Active listening and clarifying ambiguous requirements

---

## Practice Strategy:

1. **Record yourself** answering 3-4 questions to check for:
   - Confidence in delivery
   - Clarity of explanation
   - Appropriate technical depth
   - Natural flow (not robotic)

2. **Whiteboard practice**: Draw RAG architecture, agent workflow, evaluation pipeline

3. **Prepare 2-minute stories** for:
   - Your most challenging technical problem
   - A time you influenced stakeholders
   - When you had to learn something new quickly

4. **Research the company**:
   - Recent aerospace projects
   - Technology stack preferences
   - Client case studies

---

## Day-of-Interview Checklist:

‚ñ° Review this document (especially AT&T example)  
‚ñ° Check your portfolio/code samples are accessible  
‚ñ° Prepare questions for interviewer  
‚ñ° Have pen and paper ready for technical discussions  
‚ñ° Test video/audio setup 15 minutes early  
‚ñ° Close unnecessary browser tabs/applications  
‚ñ° Have water nearby  
‚ñ° Breathe and remember: you have real experience to share  

---

## Remember:

**Your technical foundation is strong** - focus on demonstrating:
- Business acumen
- Communication skills
- Domain awareness
- Problem-solving approach
- Team collaboration

You've built real systems. You understand the challenges. Now show them you can translate that into business value for aerospace clients.

**Good luck! You've got this!** üöÄ

---

*Document Version: 1.0*  
*Last Updated: January 2026*  
*Prepared for: GenAI/Agentic AI Solution Architect Interview*
